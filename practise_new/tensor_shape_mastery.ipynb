{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Shape Mastery: 30 Practice Problems\n",
    "\n",
    "**Goal**: Master tensor shape manipulation for ML interviews\n",
    "\n",
    "**Rules**:\n",
    "1. Always predict the output shape BEFORE running code\n",
    "2. Write your prediction as a comment\n",
    "3. If wrong, debug why\n",
    "4. Time yourself: aim for <30 seconds per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/t1/nxkxh_9x0mlbzst9s7g14zrm0000gp/T/ipykernel_31115/3625676295.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/amanr/miniconda/envs/ai2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def check_shape(tensor, expected_shape, question_num):\n",
    "    \"\"\"Helper to check if tensor has expected shape\"\"\"\n",
    "    actual = tuple(tensor.shape)\n",
    "    if actual == expected_shape:\n",
    "        print(f\"✅ Q{question_num}: Correct! Shape is {actual}\")\n",
    "    else:\n",
    "        print(f\"❌ Q{question_num}: Wrong! Expected {expected_shape}, got {actual}\")\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: Basic Operations (Q1-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q1: Correct! Shape is (3, 8)\n"
     ]
    }
   ],
   "source": [
    "# Q1: Basic reshape\n",
    "x = torch.randn(4, 6)\n",
    "# Reshape to (3, 8). What happens?\n",
    "# Your prediction: \n",
    "# (3,8)\n",
    "try:\n",
    "    result = x.reshape(3, 8)\n",
    "    check_shape(result, (3, 8), 1)\n",
    "except Exception as e:\n",
    "    print(f\"Q1: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q2: Correct! Shape is (2, 5, 4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0868e-02,  6.4076e-01, -1.2685e-02],\n",
       "          [ 5.2462e-01,  7.5276e-01,  3.3989e-01],\n",
       "          [-1.0495e+00, -1.3109e-03, -1.4364e+00],\n",
       "          [ 4.8354e-01,  4.7706e-01,  5.7600e-01]],\n",
       "\n",
       "         [[-3.3874e-01,  5.8325e-01,  2.4084e-01],\n",
       "          [ 1.1412e+00,  4.0476e-01,  7.1997e-01],\n",
       "          [ 6.0390e-01, -3.0360e-01, -1.1299e+00],\n",
       "          [-2.5095e+00,  7.2618e-01,  1.1415e+00]],\n",
       "\n",
       "         [[-1.3407e+00,  1.0669e+00,  1.3254e-01],\n",
       "          [ 5.1644e-02,  1.7847e-01,  4.1141e-01],\n",
       "          [-1.7223e+00, -1.4570e+00, -1.3603e-01],\n",
       "          [ 4.8800e-01,  9.1152e-02,  1.8565e-02]],\n",
       "\n",
       "         [[-5.8537e-01, -4.5015e-01,  7.6424e-01],\n",
       "          [ 7.4395e-01,  2.6491e-01,  1.9312e+00],\n",
       "          [-8.2777e-01, -1.0234e-01,  1.6354e+00],\n",
       "          [ 7.8459e-01, -3.8907e-01, -1.8058e+00]],\n",
       "\n",
       "         [[ 5.3619e-01, -1.8527e-01,  1.0950e+00],\n",
       "          [-4.8158e-01,  1.2732e+00,  1.0119e+00],\n",
       "          [ 1.3347e+00, -5.9915e-01,  6.5474e-01],\n",
       "          [ 2.8647e-02,  5.2792e-01,  9.2543e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.7534e-01, -1.9006e+00, -9.3195e-01],\n",
       "          [ 9.5846e-01, -7.3084e-01, -4.2494e-01],\n",
       "          [-7.3280e-01,  1.2888e+00, -5.7785e-01],\n",
       "          [ 1.6048e+00,  2.0265e+00, -1.5295e+00]],\n",
       "\n",
       "         [[ 1.0331e+00,  2.2858e-01, -1.5910e+00],\n",
       "          [ 1.6192e+00,  1.7482e-01,  9.4423e-01],\n",
       "          [ 1.0430e-01,  5.2296e-02,  3.2546e-01],\n",
       "          [-2.4801e+00,  3.5818e-02,  4.0487e-01]],\n",
       "\n",
       "         [[-6.8665e-01,  2.4859e-02, -1.1360e+00],\n",
       "          [ 1.4506e+00, -1.0939e+00, -1.8493e-01],\n",
       "          [ 3.4875e-01, -1.5469e+00,  2.6178e-01],\n",
       "          [-4.1754e-01,  1.2059e-01,  6.3188e-01]],\n",
       "\n",
       "         [[ 6.3681e-01, -3.4595e-01, -5.2260e-01],\n",
       "          [ 2.6948e-01, -1.6022e+00,  1.0608e+00],\n",
       "          [ 9.6759e-01,  7.5671e-01, -7.5993e-01],\n",
       "          [-1.1955e+00, -8.0566e-01,  3.1253e-01]],\n",
       "\n",
       "         [[-9.7267e-01,  2.8683e-01, -1.5933e-01],\n",
       "          [-2.1038e-01,  1.3529e+00,  2.0830e-01],\n",
       "          [-4.6569e-01,  7.7552e-01, -2.0461e+00],\n",
       "          [ 8.1234e-01, -2.0758e-01, -3.3502e-02]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2: Transpose operations\n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "# What's the shape after x.transpose(1, 3)?\n",
    "# Your prediction: \n",
    "# (2, 5, 4, 3)\n",
    "result = x.transpose(1, 3)\n",
    "check_shape(result, (2, 5, 4, 3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q3: Correct! Shape is (1, 3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2842, -0.6917, -0.5359,  0.3355],\n",
       "         [ 0.2469,  0.0324,  0.4057,  1.6181],\n",
       "         [ 0.3932, -0.2148,  1.2651, -0.3178]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3: Squeeze and unsqueeze\n",
    "x = torch.randn(1, 3, 1, 4, 1)\n",
    "# What's x.squeeze().unsqueeze(0).shape?\n",
    "# Your prediction: \n",
    "# 1, 3, 4\n",
    "result = x.squeeze().unsqueeze(0)\n",
    "check_shape(result, (1, 3, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Broadcasting addition\n",
    "x = torch.randn(2, 1, 4)\n",
    "y = torch.randn(3, 4)\n",
    "# What's (x + y).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = x + y\n",
    "check_shape(result, (2, 3, 4), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Matrix multiplication\n",
    "a = torch.randn(3, 4, 5)\n",
    "b = torch.randn(3, 5, 6)\n",
    "# What's torch.bmm(a, b).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = torch.bmm(a, b)\n",
    "check_shape(result, (3, 4, 6), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q6: Correct! Shape is (2, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9291,  0.2762, -0.5389,  0.4626, -0.8719, -0.0271, -0.3532,  1.4639,\n",
       "          1.2554, -0.7150,  0.8539,  0.5130,  0.5397,  0.5655,  0.5058,  0.2225,\n",
       "         -0.6855,  0.5636, -1.5072, -1.6107, -1.4790,  0.4323, -0.1250,  0.7821,\n",
       "         -1.5988, -0.1091,  0.7152,  0.0391,  1.3059,  0.2466, -1.9776,  0.0179,\n",
       "         -1.3793,  0.6258, -2.5850, -0.0240, -0.1222, -0.7470,  1.7093,  0.0579,\n",
       "          1.1930,  1.9373,  0.7287,  0.9809,  0.4146,  1.1566,  0.2691, -0.0366,\n",
       "          0.9733, -1.0151, -0.5419, -0.4410, -0.3136, -0.1293, -0.7150, -0.0476,\n",
       "          2.0207,  0.2539,  0.9364,  0.7122],\n",
       "        [-0.0318,  0.1016,  1.3433,  0.7133,  0.4038, -0.7140,  0.8337, -0.9585,\n",
       "          0.4536,  1.2461, -2.3065, -1.2869,  0.1799, -2.1268, -0.1341, -1.0408,\n",
       "         -0.7647, -0.0553,  1.2049, -0.9825,  0.4334, -0.7172,  1.0554, -1.4534,\n",
       "          0.4652,  0.3714, -0.0047,  0.0795,  0.3782,  0.7051, -1.7237, -0.8435,\n",
       "          0.4351,  0.2659, -0.5871,  0.0827,  0.8854,  0.1824,  0.7864, -0.0579,\n",
       "          0.5667, -0.7098, -0.4875,  0.0501,  0.8479, -0.6953,  0.3056,  0.2909,\n",
       "          0.4085, -1.2609,  0.9165, -0.0280,  0.1787, -0.0385, -0.0869, -1.1803,\n",
       "          1.5460,  0.5448,  0.9935,  0.5067]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6: View with -1\n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "# What's x.view(2, -1).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = x.view(2, -1)\n",
    "check_shape(result, (2, 60), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Permute dimensions\n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "# What's x.permute(3, 1, 0, 2).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = x.permute(3, 1, 0, 2)\n",
    "check_shape(result, (5, 3, 2, 4), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Cat along different dimensions\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = torch.randn(2, 5, 4)\n",
    "# What's torch.cat([x, y], dim=1).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = torch.cat([x, y], dim=1)\n",
    "check_shape(result, (2, 8, 4), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: Stack vs Cat\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)\n",
    "z = torch.randn(2, 3)\n",
    "# What's torch.stack([x, y, z], dim=1).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = torch.stack([x, y, z], dim=1)\n",
    "check_shape(result, (2, 3, 3), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10: Advanced indexing\n",
    "x = torch.randn(4, 5, 6)\n",
    "# What's x[:, [1, 3], :].shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = x[:, [1, 3], :]\n",
    "check_shape(result, (4, 2, 6), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Neural Network Operations (Q11-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11: Linear layer\n",
    "batch_size, seq_len, d_model = 4, 10, 512\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "linear = nn.Linear(d_model, 256)\n",
    "# What's linear(x).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = linear(x)\n",
    "check_shape(result, (4, 10, 256), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q12: Embedding layer\n",
    "vocab_size, embed_dim = 1000, 128\n",
    "seq_len, batch_size = 20, 8\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len)) # 8, 20\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "# What's embedding(token_ids).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = embedding(token_ids)\n",
    "check_shape(result, (8, 20, 128), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13: Multi-head attention setup\n",
    "batch_size, seq_len, d_model = 2, 16, 512\n",
    "num_heads = 8\n",
    "x = torch.randn(batch_size, seq_len, d_model) # 2, 16, 512\n",
    "# Split for multi-head: reshape to (batch, seq, heads, head_dim)\n",
    "# What's the head_dim and final shape after transpose?\n",
    "# Your prediction: \n",
    "\n",
    "head_dim = d_model // num_heads\n",
    "x_heads = x.view(batch_size, seq_len, num_heads, head_dim) # (2, 16, 8, 64)\n",
    "x_transposed = x_heads.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
    "check_shape(x_transposed, (2, 8, 16, 64), 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14: Attention scores\n",
    "batch_size, num_heads, seq_len, head_dim = 2, 8, 16, 64\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "# What's torch.matmul(q, k.transpose(-2, -1)).shape?\n",
    "# Your prediction: 2, 8, 16, 64 | 2, 8, 64, 16\n",
    "# 2, 8, 16, 16\n",
    "\n",
    "scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "check_shape(scores, (2, 8, 16, 16), 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q15: Causal mask\n",
    "seq_len = 10\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "# You have attention scores (4, 6, 10, 10)\n",
    "# How does mask broadcast when you do scores.masked_fill(mask == 0, -float('inf'))?\n",
    "# Your prediction: \n",
    "\n",
    "scores = torch.randn(4, 6, 10, 10)\n",
    "result = scores.masked_fill(mask == 0, -float('inf'))\n",
    "check_shape(result, (4, 6, 10, 10), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q16: Conv1D for text\n",
    "batch_size, seq_len, embed_dim = 8, 100, 256\n",
    "x = torch.randn(batch_size, embed_dim, seq_len)  # 8, 256, 100\n",
    "conv = nn.Conv1d(embed_dim, 512, kernel_size=3, padding=1)\n",
    "# What's conv(x).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = conv(x)\n",
    "check_shape(result, (8, 512, 100), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q17: Global average pooling\n",
    "x = torch.randn(4, 512, 28, 28)  # Image features\n",
    "# What's F.adaptive_avg_pool2d(x, (1, 1)).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "check_shape(result, (4, 512, 1, 1), 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q18: Batch normalization\n",
    "x = torch.randn(32, 256, 14, 14)\n",
    "bn = nn.BatchNorm2d(256)\n",
    "# What's bn(x).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = bn(x)\n",
    "check_shape(result, (32, 256, 14, 14), 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q19: Layer normalization\n",
    "x = torch.randn(4, 10, 512)  # (batch, seq, features)\n",
    "ln = nn.LayerNorm(512)\n",
    "# What's ln(x).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = ln(x)\n",
    "check_shape(result, (4, 10, 512), 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q20: Dropout during training\n",
    "x = torch.randn(8, 16, 256)\n",
    "dropout = nn.Dropout(0.1)\n",
    "# What's dropout(x).shape during training?\n",
    "# Your prediction: \n",
    "\n",
    "dropout.train()\n",
    "result = dropout(x)\n",
    "check_shape(result, (8, 16, 256), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: Advanced Operations (Q21-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q21: Einstein summation\n",
    "a = torch.randn(4, 3, 5)\n",
    "b = torch.randn(4, 5, 7)\n",
    "# What's torch.einsum('bij,bjk->bik', a, b).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = torch.einsum('bij,bjk->bik', a, b)\n",
    "check_shape(result, (4, 3, 7), 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q22: Cross attention\n",
    "# Query from decoder, Key/Value from encoder\n",
    "q = torch.randn(2, 8, 20, 64)  # (batch, heads, tgt_len, head_dim)\n",
    "k = torch.randn(2, 8, 30, 64)  # (batch, heads, src_len, head_dim)\n",
    "v = torch.randn(2, 8, 30, 64)\n",
    "# What's the attention scores shape: q @ k.transpose(-2, -1)?\n",
    "# Your prediction: \n",
    "\n",
    "scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "check_shape(scores, (2, 8, 20, 30), 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q23: Grouped convolution\n",
    "x = torch.randn(8, 64, 32, 32)\n",
    "conv = nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=8)\n",
    "# What's conv(x).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = conv(x)\n",
    "check_shape(result, (8, 128, 32, 32), 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q24: Positional encoding addition\n",
    "batch_size, seq_len, d_model = 4, 50, 512\n",
    "tokens = torch.randn(batch_size, seq_len, d_model)\n",
    "pos_encoding = torch.randn(1, seq_len, d_model)  # Learned positional encoding\n",
    "# What's (tokens + pos_encoding).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = tokens + pos_encoding\n",
    "check_shape(result, (4, 50, 512), 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q25: Gather operation\n",
    "x = torch.randn(3, 5, 4)\n",
    "indices = torch.tensor([[0, 2], [1, 4], [0, 3]]) # (3, 2)\n",
    "# What's torch.gather(x, 1, indices.unsqueeze(-1).expand(-1, -1, 4)).shape?\n",
    "# Your prediction: (3, 2, 4)\n",
    "\n",
    "expanded_indices = indices.unsqueeze(-1).expand(-1, -1, 4)\n",
    "result = torch.gather(x, 1, expanded_indices)\n",
    "check_shape(result, (3, 2, 4), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q26: Selected 8 elements from 8 True mask values\n",
      "✅ Q26: Correct! Shape is torch.Size([8]) (1D with 8 elements)\n"
     ]
    }
   ],
   "source": [
    "# Q26: Masked select\n",
    "x = torch.randn(4, 6)\n",
    "mask = torch.randint(0, 2, (4, 6)).bool()\n",
    "# What's x[mask].shape? (Note: result is 1D!)\n",
    "# Your prediction: \n",
    "result = x[mask]\n",
    "# Shape will be (N,) where N is number of True values in mask\n",
    "print(f\"Q26: Selected {result.shape[0]} elements from {mask.sum().item()} True mask values\")\n",
    "print(f\"✅ Q26: Correct! Shape is {result.shape} (1D with {result.shape[0]} elements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q27: Repeat and tile\n",
    "x = torch.randn(2, 3)\n",
    "# What's x.repeat(4, 1, 2).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = x.repeat(4, 1, 2)\n",
    "check_shape(result, (4, 2, 6), 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q28: Advanced broadcasting\n",
    "a = torch.randn(8, 1, 6, 1)\n",
    "b = torch.randn(7, 1, 5)\n",
    "# What's (a * b).shape?\n",
    "# Your prediction: \n",
    "\n",
    "result = a * b\n",
    "check_shape(result, (8, 7, 6, 5), 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q29: Unfold operation (sliding window)\n",
    "x = torch.randn(1, 1, 10)  # (batch, channels, length)\n",
    "# What's F.unfold(x.unsqueeze(-1), kernel_size=(3, 1), padding=(1, 0)).shape?\n",
    "# Your prediction: \n",
    "\n",
    "x_2d = x.unsqueeze(-1)  # Make it (1, 1, 10, 1) for unfold\n",
    "result = F.unfold(x_2d, kernel_size=(3, 1), padding=(1, 0))\n",
    "check_shape(result, (1, 3, 10), 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q30: Complex reshape puzzle\n",
    "x = torch.randn(2, 3, 4, 5, 6)\n",
    "# Flatten last 3 dims, then swap first two dims\n",
    "# What's x.view(2, 3, -1).transpose(0, 1).shape?\n",
    "# Your prediction: \n",
    "\n",
    "flattened = x.view(2, 3, -1)  # Last 3 dims: 4*5*6 = 120\n",
    "result = flattened.transpose(0, 1)\n",
    "check_shape(result, (3, 2, 120), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Common Interview Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 1: Implement scaled dot-product attention from scratch\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    q, k, v: (batch, heads, seq_len, head_dim)\n",
    "    mask: (seq_len, seq_len) or broadcastable\n",
    "    \"\"\"\n",
    "    # Your implementation here - predict all intermediate shapes!\n",
    "    d_k = q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    print(f\"Scores shape: {scores.shape}\")  # Should be (batch, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Step 2: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\")  # Same as scores\n",
    "    \n",
    "    # Step 4: Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "    \n",
    "    # Step 5: Apply attention to values\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be (batch, heads, seq_len, head_dim)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# Test it\n",
    "batch, heads, seq_len, head_dim = 2, 8, 16, 64\n",
    "q = torch.randn(batch, heads, seq_len, head_dim)\n",
    "k = torch.randn(batch, heads, seq_len, head_dim)\n",
    "v = torch.randn(batch, heads, seq_len, head_dim)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(q, k, v)\n",
    "print(f\"\\nFinal shapes - Output: {output.shape}, Weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Batch processing with variable lengths\n",
    "# Common interview scenario: handle sequences of different lengths\n",
    "\n",
    "def create_padding_mask(lengths, max_len):\n",
    "    \"\"\"Create mask for variable length sequences\"\"\"\n",
    "    batch_size = len(lengths)\n",
    "    mask = torch.arange(max_len).expand(batch_size, max_len) < torch.tensor(lengths).unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "# Example: batch with sequences of lengths [5, 8, 3]\n",
    "lengths = [5, 8, 3]\n",
    "max_len = 10\n",
    "mask = create_padding_mask(lengths, max_len)\n",
    "\n",
    "print(f\"Mask shape: {mask.shape}\")  # Should be (3, 10)\n",
    "print(f\"Mask:\\n{mask.int()}\")\n",
    "\n",
    "# Use mask to zero out padded positions\n",
    "x = torch.randn(3, 10, 256)  # (batch, seq, features)\n",
    "x_masked = x * mask.unsqueeze(-1).float()\n",
    "print(f\"Masked input shape: {x_masked.shape}\")  # Should be (3, 10, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Always write down tensor shapes as comments\n",
    "2. Understand broadcasting rules (right-align dimensions)\n",
    "3. Matrix multiplication: last dim of A = second-to-last dim of B\n",
    "4. Practice predicting shapes before running code\n",
    "5. Use `.shape` liberally when debugging\n",
    "\n",
    "**Common Patterns:**\n",
    "- Multi-head attention: `(batch, seq, dim) → (batch, heads, seq, head_dim)`\n",
    "- Attention scores: `(batch, heads, seq_q, head_dim) @ (batch, heads, head_dim, seq_k) → (batch, heads, seq_q, seq_k)`\n",
    "- Broadcasting: smaller tensors expand to match larger ones\n",
    "- Masking: use `.masked_fill()` for attention masks\n",
    "\n",
    "**Interview Tips:**\n",
    "- Verbalize your shape reasoning out loud\n",
    "- Start with simple examples and build up\n",
    "- Don't be afraid to use `.shape` to verify\n",
    "- Practice these patterns until they're automatic!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
