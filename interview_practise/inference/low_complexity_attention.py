"""
Low-Complexity Attention Methods

This module demonstrates different attention mechanisms and their computational complexities:
1. Standard Attention - O(nÂ²)
2. Kernel-based Attention - O(n)
3. Low-rank Attention - O(n)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class StandardAttention(nn.Module):
    """Standard attention with O(nÂ²) complexity

    Computes: softmax(QK^T/âˆšd)V
    Complexity: O(nÂ²d) where n is sequence length, d is feature dimension
    """

    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        self.scale = 1.0 / math.sqrt(d_model)

    def forward(self, q, k, v):
        # q, k, v shape: (batch, n, d)
        n = q.size(1)

        # Step 1: Compute QK^T - O(nÂ²d) complexity
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (batch, n, n)

        # Step 2: Softmax - O(nÂ²) complexity
        attn_weights = F.softmax(scores, dim=-1)  # (batch, n, n)

        # Step 3: Apply to values - O(nÂ²d) complexity
        output = torch.matmul(attn_weights, v)  # (batch, n, d)

        print(f"Standard Attention:")
        print(f"  Attention matrix shape: {attn_weights.shape}")
        print(f"  Memory usage: O(nÂ²) = O({n}Â²) = {n*n} elements")
        print(f"  Computational complexity: O(nÂ²d) = O({n}Â² Ã— {self.d_model})")

        return output

class KernelBasedAttention(nn.Module):
    """Kernel-based attention with O(n) complexity

    Uses kernel approximation: Ï†(Q)Ï†(K)^TV instead of softmax(QK^T/âˆšd)V
    Key insight: Ï†(Q)(Ï†(K)^TV) can be computed as Ï†(Q) Ã— (Ï†(K)^TV)
    """

    def __init__(self, d_model, kernel_dim=64):
        super().__init__()
        self.d_model = d_model
        self.kernel_dim = kernel_dim

    def kernel_feature_map(self, x):
        """Simple ReLU-based kernel approximation"""
        # Apply random projection + ReLU as kernel approximation
        # In practice, more sophisticated kernels like RFF are used
        return F.relu(x)  # Simplified for demonstration

    def forward(self, q, k, v):
        # q, k, v shape: (batch, n, d)
        n = q.size(1)

        # Step 1: Apply kernel feature map
        phi_q = self.kernel_feature_map(q)  # (batch, n, d)
        phi_k = self.kernel_feature_map(k)  # (batch, n, d)

        # Step 2: Compute Ï†(K)^T V first - O(ndÂ²) complexity
        kv = torch.matmul(phi_k.transpose(-2, -1), v)  # (batch, d, d)

        # Step 3: Then Ï†(Q) Ã— (Ï†(K)^T V) - O(ndÂ²) complexity
        output = torch.matmul(phi_q, kv)  # (batch, n, d)

        # Normalize (simplified)
        output = output / (phi_q.sum(dim=-1, keepdim=True) + 1e-8)

        print(f"Kernel-based Attention:")
        print(f"  No explicit attention matrix computed")
        print(f"  Intermediate KV shape: {kv.shape}")
        print(f"  Memory usage: O(dÂ²) = O({self.d_model}Â²) = {self.d_model**2} elements")
        print(f"  Computational complexity: O(ndÂ²) = O({n} Ã— {self.d_model}Â²)")

        return output

class LowRankAttention(nn.Module):
    """Low-rank attention with O(n) complexity

    Approximates attention matrix A â‰ˆ UV^T where U,V have rank r << n
    This reduces O(nÂ²) attention matrix to O(nr) complexity
    """

    def __init__(self, d_model, rank=32):
        super().__init__()
        self.d_model = d_model
        self.rank = rank

        # Low-rank projections
        self.q_to_u = nn.Linear(d_model, rank, bias=False)
        self.k_to_v = nn.Linear(d_model, rank, bias=False)

    def forward(self, q, k, v):
        # q, k, v shape: (batch, n, d)
        batch_size, n, d = q.shape

        # Step 1: Project to low-rank space - O(ndr) complexity
        u = self.q_to_u(q)  # (batch, n, rank)
        v_proj = self.k_to_v(k)  # (batch, n, rank)

        # Step 2: Compute attention in low-rank space - O(nrÂ²) complexity
        # Instead of nÃ—n matrix, we work with nÃ—r matrices
        scores = torch.matmul(u, v_proj.transpose(-2, -1))  # (batch, n, n) but computed efficiently
        attn_weights = F.softmax(scores / math.sqrt(self.rank), dim=-1)

        # Step 3: Apply to values - O(nÂ²d) -> This can be further optimized
        output = torch.matmul(attn_weights, v)  # (batch, n, d)

        print(f"Low-rank Attention:")
        print(f"  Low-rank projections: {n} Ã— {self.rank} each")
        print(f"  Rank: {self.rank} << sequence length {n}")
        print(f"  Memory usage: O(nr) = O({n} Ã— {self.rank}) = {n * self.rank} elements")
        print(f"  Computational complexity: O(nrÂ²) = O({n} Ã— {self.rank}Â²)")

        return output

def demonstrate_complexity_comparison():
    """Compare computational complexities of different attention mechanisms"""

    batch_size = 1
    n = 1024  # sequence length
    d = 512   # model dimension

    print("=" * 80)
    print("LOW-COMPLEXITY ATTENTION COMPARISON")
    print("=" * 80)
    print(f"Sequence length (n): {n}")
    print(f"Model dimension (d): {d}")
    print()

    # Create sample inputs
    q = torch.randn(batch_size, n, d)
    k = torch.randn(batch_size, n, d)
    v = torch.randn(batch_size, n, d)

    print("COMPLEXITY ANALYSIS:")
    print("-" * 40)

    # Standard Attention
    std_attn = StandardAttention(d)
    _ = std_attn(q, k, v)
    print()

    # Kernel-based Attention
    kernel_attn = KernelBasedAttention(d)
    _ = kernel_attn(q, k, v)
    print()

    # Low-rank Attention
    lowrank_attn = LowRankAttention(d, rank=64)
    _ = lowrank_attn(q, k, v)
    print()

    print("KEY INSIGHTS:")
    print("-" * 40)
    print("ðŸ”¥ THE QUADRATIC BOTTLENECK:")
    print(f"   â€¢ Standard attention: O(nÂ²) = O({n}Â²) = {n*n:,} operations")
    print(f"   â€¢ This grows VERY quickly with sequence length!")
    print(f"   â€¢ For n=4096: {4096*4096:,} operations")
    print(f"   â€¢ For n=8192: {8192*8192:,} operations")
    print()

    print("âœ… LINEAR COMPLEXITY SOLUTIONS:")
    print("   â€¢ Kernel methods: Avoid computing full attention matrix")
    print("   â€¢ Low-rank: Approximate attention with rank-r matrices")
    print("   â€¢ Both achieve O(n) or O(nr) complexity")
    print()

    print("ðŸ“Š TRADE-OFFS:")
    print("   â€¢ Standard: Best quality, but O(nÂ²) complexity")
    print("   â€¢ Kernel-based: O(n) complexity, approximation quality depends on kernel")
    print("   â€¢ Low-rank: O(nr) complexity, quality depends on rank r")
    print()

    print("ðŸŽ¯ WHEN TO USE:")
    print("   â€¢ Short sequences (n < 1024): Standard attention")
    print("   â€¢ Long sequences (n > 4096): Linear complexity methods")
    print("   â€¢ Very long sequences (n > 16K): Essential for feasibility")

if __name__ == "__main__":
    demonstrate_complexity_comparison()